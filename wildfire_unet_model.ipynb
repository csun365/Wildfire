{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks, Sequential, Input, Model\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, Add, BatchNormalization, Conv2D, SeparableConv2D, Conv2DTranspose, UpSampling2D, MaxPooling2D, Flatten, concatenate\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import math\n",
    "import scipy\n",
    "import pickle\n",
    "import gc \n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Band 0 - Coastal aerosol\n",
    "\n",
    "Band 1 - Blue\n",
    "\n",
    "Band 2 - Green\n",
    "\n",
    "Band 3 - Red\n",
    "\n",
    "Band 4 - Near Infrared (NIR)\n",
    "\n",
    "Band 5 - SWIR 1\n",
    "\n",
    "Band 6 - SWIR 2\n",
    "\n",
    "Band 7 - Cirrus\n",
    "\n",
    "Band 8 - Thermal Infrared (TIRS) 1\n",
    "\n",
    "Band 9 - Thermal Infrared (TIRS) 2\n",
    "\n",
    "(Panchromatic Band is Excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validation data from Ecuador/Galapagos\n",
    "fire_path = \"\"\n",
    "nonfire_path = \"\"\n",
    "mask_path = \"\"\n",
    "\n",
    "fires = pickle.load(open(fire_path, \"rb\"))\n",
    "nonfires = pickle.load(open(nonfire_path, \"rb\"))\n",
    "masks = pickle.load(open(mask_path, \"rb\"))\n",
    "masks = masks.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data from Guyana/Suriname\n",
    "test_fire_path = \"\"\n",
    "test_mask_path = \"\"\n",
    "\n",
    "fires_test = pickle.load(open(test_fire_path, \"rb\"))\n",
    "masks_test = pickle.load(open(test_mask_path, \"rb\"))\n",
    "masks_test = masks_test.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_idxs(data):\n",
    "    idxs = np.argwhere(np.min(data.reshape(data.shape[0], 128*128*10), axis=1) != 0)\n",
    "    idxs = idxs.reshape(idxs.shape[0])\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_fire = return_idxs(fires)\n",
    "fires = fires[idxs_fire]\n",
    "masks = masks[idxs_fire]\n",
    "\n",
    "idxs_nonfire = return_idxs(nonfires)\n",
    "nonfires = nonfires[idxs_nonfire]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_v, y_t, y_v = train_test_split(fires, masks, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Normalization\n",
    "X_t_mins = np.min(np.min(np.min(X_t, axis=0), axis=0), axis=0) \n",
    "X_t_maxs = np.max(np.max(np.max(X_t, axis=0), axis=0), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = (X_t - X_t_mins) / (X_t_maxs - X_t_mins)\n",
    "X_v = (X_v - X_t_mins) / (X_t_maxs - X_t_mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing Test Data\n",
    "fires_test = (fires_test - X_t_mins) / (X_t_maxs - X_t_mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_m = fires.shape[0]\n",
    "nonfires_m = nonfires.shape[0]\n",
    "tot = fires_m + nonfires_m\n",
    "print(\"Number of fires:\\t\" + str(fires_m))\n",
    "print(\"Number of nonfires:\\t\" + str(nonfires_m))\n",
    "print(\"Total:\\t\" + str(tot))\n",
    "print(\"Percentage that are fire:\\t\" + str(fires_m/tot))\n",
    "print(\"Percentage that are nonfire:\\t\" + str(nonfires_m/tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a sense of the number of fires in each image\n",
    "num_clusters = []\n",
    "\n",
    "for i in range(500):\n",
    "    fires = np.array(list(np.where(masks[i] == 1))) # 2 x m array where m is the number of pixels that equal 1\n",
    "    cost = np.inf\n",
    "    clusters = 0\n",
    "    cost_threshold = np.sum(masks[i].reshape(masks[ic].shape[0], masks[i].shape[1]**2), axis=1) / 3\n",
    "    while cost * 10 > cost_threshold:\n",
    "        clusters += 1\n",
    "        kmeans = KMeans(clusters)\n",
    "        kmeans.fit(fires.T)\n",
    "        cost = kmeans.inertia_\n",
    "    num_clusters.append(clusters)\n",
    "print(num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of pixels with active fires\n",
    "# Used for loss function weighting \n",
    "num_fires = np.sum(np.sum(masks, axis=-1), axis=-1)\n",
    "plt.hist(num_fires,bins=50)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize single 128 x 128 x 3 image for plotting\n",
    "def plot_single(arr):\n",
    "    epsilon = 1e-4\n",
    "    reshaped_arr = arr.reshape(arr.shape[0] * arr.shape[1], arr.shape[2])\n",
    "    arr_min = np.min(reshaped_arr, axis=0)\n",
    "    arr_max = np.max(reshaped_arr, axis=0)\n",
    "    reshaped_arr_norm = (reshaped_arr - arr_min) / (arr_max - arr_min + epsilon) \n",
    "    img = reshaped_arr_norm.reshape(arr.shape[0], arr.shape[1], arr.shape[2])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_model():\n",
    "    inputs = Input(shape=(128, 128, 3))\n",
    "    conv1 = Conv2D(32, (3, 3), padding='same')(inputs)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation(\"relu\")(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(64, (3, 3), padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation(\"relu\")(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation(\"relu\")(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation(\"relu\")(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation(\"relu\")(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4 = Conv2D(256, (3, 3), padding='same')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation(\"relu\")(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation(\"relu\")(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    up5 = concatenate([Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(conv4), conv3], axis=3)\n",
    "    conv5 = Conv2D(128, (3, 3), padding='same')(up5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation(\"relu\")(conv5)\n",
    "    conv5 = Conv2D(128, (3, 3), padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation(\"relu\")(conv5)\n",
    "\n",
    "    up6 = concatenate([Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(conv5), conv2], axis=3)\n",
    "    conv6 = Conv2D(64, (3, 3), padding='same')(up6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation(\"relu\")(conv6)\n",
    "    conv6 = Conv2D(64, (3, 3), padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation(\"relu\")(conv6)\n",
    "\n",
    "    up7 = concatenate([Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(conv6), conv1], axis=3)\n",
    "    conv7 = Conv2D(32, (3, 3), padding='same')(up7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation(\"relu\")(conv7)\n",
    "    conv7 = Conv2D(16, (3, 3), padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation(\"relu\")(conv7)\n",
    "  \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(conv7)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_unet_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='unet_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting for class imbalance\n",
    "weight_pos = (masks.shape[0]*masks.shape[1]*masks.shape[2])/np.sum(num_fires) - 1 # weight_pos = 2095.5433356044036\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    y_true = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    epsilon = 1e-4\n",
    "    error = - (weight_pos * y_true * tf.math.log(y_pred + epsilon) + (1 - y_true) * tf.math.log(1 - y_pred + epsilon))\n",
    "    loss = tf.math.reduce_mean(error, axis=-1)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    y_pred = tf.cast(tf.math.greater_equal(y_pred, 0.5), tf.float32)\n",
    "    y_true = Flatten()(y_true)\n",
    "    y_pred = Flatten()(y_pred)\n",
    "    epsilon = 1e-4\n",
    "    tp = tf.math.logical_and((y_true == 1), (y_pred == 1))\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    tp = tf.math.reduce_sum(tp, axis=1)\n",
    "    fp = tf.math.logical_and((y_true == 0), (y_pred == 1))\n",
    "    fp = tf.cast(fp, tf.float32)\n",
    "    fp = tf.math.reduce_sum(fp, axis=1)\n",
    "    fn = tf.math.logical_and((y_true == 1), (y_pred == 0))\n",
    "    fn = tf.cast(fn, tf.float32)\n",
    "    fn = tf.math.reduce_sum(fn, axis=1)\n",
    "    \n",
    "    return tp / (tp + 0.2 * fp + 0.8 * fn + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    y_pred = tf.cast(tf.math.greater_equal(y_pred, 0.5), tf.float32)\n",
    "    y_true = Flatten()(y_true)\n",
    "    y_pred = Flatten()(y_pred)\n",
    "    epsilon = 1e-4\n",
    "    tp = tf.math.logical_and((y_true == 1), (y_pred == 1))\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    tp = tf.math.reduce_sum(tp, axis=1)\n",
    "    fp = tf.math.logical_and((y_true == 0), (y_pred == 1))\n",
    "    fp = tf.cast(fp, tf.float32)\n",
    "    fp = tf.math.reduce_sum(fp, axis=1)\n",
    "    fn = tf.math.logical_and((y_true == 1), (y_pred == 0))\n",
    "    fn = tf.cast(fn, tf.float32)\n",
    "    fn = tf.math.reduce_sum(fn, axis=1)\n",
    "    \n",
    "    return 2 * tp / (2 * tp + fp + fn + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    y_pred = tf.cast(tf.math.greater_equal(y_pred, 0.5), tf.float32)\n",
    "    y_true = Flatten()(y_true)\n",
    "    y_pred = Flatten()(y_pred)\n",
    "    epsilon = 1e-4\n",
    "    tp = tf.math.logical_and((y_true == 1), (y_pred == 1))\n",
    "    tp = tf.cast(tp, tf.float32)\n",
    "    tp = tf.math.reduce_sum(tp)\n",
    "    fp = tf.math.logical_and((y_true == 0), (y_pred == 1))\n",
    "    fp = tf.cast(fp, tf.float32)\n",
    "    fp = tf.math.reduce_sum(fp)\n",
    "    fn = tf.math.logical_and((y_true == 1), (y_pred == 0))\n",
    "    fn = tf.cast(fn, tf.float32)\n",
    "    fn = tf.math.reduce_sum(fn)\n",
    "    tn = tf.math.logical_and((y_true == 0), (y_pred == 0))\n",
    "    tn = tf.cast(tn, tf.float32)\n",
    "    tn = tf.math.reduce_sum(tn)\n",
    "    \n",
    "    return tp, fp, fn, tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha0 = 5e-4\n",
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.1\n",
    "    return alpha0 * math.exp(-k * epoch)\n",
    "\n",
    "alpha_schedule = tf.keras.callbacks.LearningRateScheduler(lr_exp_decay)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=alpha_schedule)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[f2_score, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_t, y_t, validation_data=(X_v, y_v), batch_size=32, epochs=100, callbacks=[alpha_schedule])\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "# Rolling average\n",
    "# val_loss = history.history[\"val_loss\"][:9] + list(pd.Series(history.history[\"val_loss\"]).rolling(10).mean().dropna())\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "# plt.ylim(0,0.05)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"f2_score\"])\n",
    "# Rolling average\n",
    "# val_f2 = history.history[\"val_f2_score\"][:9] + list(pd.Series(history.history[\"val_f2_score\"]).rolling(10).mean().dropna())\n",
    "plt.plot(history.history[\"val_f2_score\"])\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"f1_score\"])\n",
    "# Rolling average\n",
    "# val_f1 = history.history[\"val_f1_score\"][:9] + list(pd.Series(history.history[\"val_f1_score\"]).rolling(10).mean().dropna())\n",
    "plt.plot(history.history[\"val_f1_score\"])\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('unet_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on validation set\n",
    "y_pred = model.predict(X_v, batch_size=32)\n",
    "y_pred = (y_pred >= 0.5).astype(\"int\")\n",
    "evaluate(y_v, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"../unet_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on test set (Guyana/Suriname images)\n",
    "evaluate(masks_test, model.predict(fires_test, batch_size=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffler = np.random.permutation(2000)\n",
    "nonfires_x_pred = nonfires[:,:,:,[2,5,6]]\n",
    "nonfires_x_pred = nonfires_x_pred[shuffler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on 2000 nonfire images (ideally should predict every pixel = 0)\n",
    "evaluate(np.zeros((2000,128,128)), model.predict(nonfires_x_pred, batch_size=32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
